# Client-Side-Audio-Emotion-Detection---Phase-2
A **fully browser-based, real-time emotion detection system** built using **TensorFlow.js**, designed to run **entirely on the client side** with **no backend inference**.

This project represents **Phase-2++**, a stability-focused redesign after identifying deployment limitations in CNN-based browser models.

---

## ğŸš€ Project Overview

**Goal:**  
Build a **browser-deployable emotion detection system** that performs real-time inference on microphone audio using a **Dense-only neural network** and **MFCC features**, fully compatible with TensorFlow.js.

**Key Principle:**  
> If it doesnâ€™t run safely in the browser, it doesnâ€™t belong in Phase-2++.

---

## âœ… Key Features

- ğŸ§ Real-time microphone audio capture (Web Audio API)
- ğŸ§  Dense-only neural network (TFJS LayersModel)
- ğŸ“Š 13-dimensional MFCC feature input
- âš¡ 100% client-side inference (no backend, no APIs)
- ğŸ¨ Color-coded emotion confidence bars
- ğŸ” Prediction smoothing (EMA)
- ğŸ”‡ Silence detection & gating
- â± FPS / latency throttling for performance
- ğŸŒ Works in modern browsers (Chrome, Edge, Firefox)

---

## ğŸ§  Supported Emotions

The current model is trained to classify **4 emotions**:

| Emotion  | Description |
|--------|-------------|
| Angry   | High-energy negative affect |
| Happy   | Positive / excited affect |
| Sad     | Low-energy negative affect |
| Neutral | Baseline / calm speech |

---

## ğŸ“ Repository Structure

Client-Side-Emotion-Detection/
â”œâ”€â”€ public/
â”‚ â”œâ”€â”€ index.html # Main UI
â”‚ â”œâ”€â”€ main.js # Audio capture + inference logic
â”‚ â””â”€â”€ mfcc.js # MFCC feature extraction (13-D)
â”œâ”€â”€ model/
â”‚ â”œâ”€â”€ model.json # TFJS LayersModel
â”‚ â””â”€â”€ group1-shard1of1.bin
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

yaml
Copy code

---

## â–¶ï¸ How to Run Locally

### âš ï¸ Important
Do **NOT** open `index.html` directly using `file://`.  
TensorFlow.js requires an HTTP server.

---

### Option 1: Python HTTP Server (Recommended)

From the project root:

```bash
python -m http.server 8000
Then open in browser:

bash
Copy code
http://localhost:8000/public/index.html
Option 2: VS Code Live Server
Install Live Server extension

Right-click public/index.html

Select â€œOpen with Live Serverâ€

ğŸ§ª How to Use
Click Initialize

Allow microphone permission

Click Start Detection

Speak normally (try different emotions)

Observe:

Live emotion label

Confidence percentage

Color-coded bars

Silence is automatically detected and gated.

ğŸ§© Technical Design (Phase-2++ Rules)
This phase intentionally avoids browser-unsafe designs.

âœ” Whatâ€™s Used
Dense neural network

Single-frame MFCC (no time stacking)

TensorFlow.js LayersModel

Web Audio API

âŒ Whatâ€™s Avoided
CNNs / spectrograms

RNNs / LSTMs

Backend inference

tfjs-node

SavedModel / GraphModel in browser

ğŸ“Š Dataset & Training (Offline)
Dataset: CREMA-D

Feature: 13-dimensional MFCC

Training: Python (TensorFlow / Keras)

Export: Keras â†’ TensorFlow.js LayersModel

Training scripts and raw datasets are intentionally excluded from this repo to keep it lightweight and deployment-focused.

âš ï¸ Known Limitations
Emotion accuracy depends on microphone quality

No temporal context (single-frame inference by design)

Model size and complexity intentionally limited for browser safety

These are acceptable tradeoffs for Phase-2++.

ğŸ”® Future Roadmap
Phase-3: Hybrid CNN â†’ Dense (distilled features, browser-safe)

Phase-4: Production hardening (WebWorkers, WASM, mobile optimization)

Emotion timeline visualization

Multi-language emotion labels

ğŸ Phase Status
âœ… Phase-2++ completed successfully
âœ… Browser inference stable
âœ… Architecture validated

ğŸ“œ License
This project is intended for educational and research purposes.
Dataset licenses (e.g., CREMA-D) apply to training data separately.

ğŸ™Œ Acknowledgements
TensorFlow.js team

CREMA-D dataset authors

Web Audio API contributors
